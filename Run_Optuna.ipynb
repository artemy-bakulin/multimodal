{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47c79814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/artemy/multimodal_proj/nb/utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "#import time\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.swa_utils import AveragedModel\n",
    "#from tqdm import tqdm\n",
    "\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "import utils\n",
    "\n",
    "from importlib import reload\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d5a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_n = 0\n",
    "device = torch.device(f\"cuda:{cuda_n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc23005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Universal_Model(nn.Module, utils.HyperParameters):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 output_dim: int,\n",
    "                 layers_sizes = [512] * 4,\n",
    "                 concat_pos = 4,\n",
    "                 n_of_layers_to_concat = 3,\n",
    "                 dropout = 1,\n",
    "                 device = 'cpu',\n",
    "                 **kwargs \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.layers_sizes = [self.input_dim] + self.layers_sizes + [self.output_dim]\n",
    "        self.n_layers = len(self.layers_sizes) - 1\n",
    "        modules = []\n",
    "        for i in range(self.n_layers):\n",
    "            if i != self.concat_pos:\n",
    "                input_dim = self.layers_sizes[i]\n",
    "            else:\n",
    "                input_dim = sum(self.layers_sizes[self.concat_pos-self.n_of_layers_to_concat+1:self.concat_pos+1])\n",
    "            output_dim = self.layers_sizes[i + 1]\n",
    "            if i < self.n_layers and dropout != 1:\n",
    "                modules.append(nn.Dropout1d(dropout))\n",
    "            modules.append(nn.Linear(input_dim, output_dim))\n",
    "            if i < self.n_layers - 1:\n",
    "                modules.append(nn.BatchNorm1d(num_features=output_dim))          \n",
    "                modules.append(nn.SiLU())\n",
    "        self.net = nn.Sequential(*modules)\n",
    "        self.net.apply(utils.init_weights)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        fc_layer_n = 0\n",
    "        layer_outputs = []\n",
    "        for module in self.net:\n",
    "            if fc_layer_n == self.concat_pos and isinstance(module, nn.Linear):\n",
    "                x = torch.concat(layer_outputs[-self.n_of_layers_to_concat:], 1)\n",
    "            x = module(x)\n",
    "            if isinstance(module, nn.SiLU):\n",
    "                layer_outputs.append(x)\n",
    "            if isinstance(module, nn.Linear):\n",
    "                fc_layer_n += 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e5d6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, train=True, cite=True):\n",
    "    inputs_file = data_dir\n",
    "    targets_file = data_dir\n",
    "\n",
    "    if cite: \n",
    "        inputs_file += 'cite_gex_'\n",
    "        targets_file += 'cite_adt_'\n",
    "    else:\n",
    "        inputs_file += 'atac_'\n",
    "        targets_file += 'gex_'\n",
    "    if train:\n",
    "        inputs_file += 'train'\n",
    "        targets_file += 'train'\n",
    "    else:\n",
    "        inputs_file += 'test'\n",
    "        targets_file += 'test'\n",
    "        \n",
    "    data = {}\n",
    "    inputs = utils.load_sparse_data(inputs_file + '_svd.sparse.npz')\n",
    "    data['inputs'] = inputs\n",
    "    targets = utils.load_sparse_data(targets_file + '.sparse.npz')\n",
    "    data['targets'] = targets\n",
    "        \n",
    "    if cite:\n",
    "        cycle_levels = utils.load_sparse_data(inputs_file + '_cycle.sparse.npz')\n",
    "        data['cycle'] = cycle_levels\n",
    "        cd_levels = utils.load_sparse_data(inputs_file + '_cd.sparse.npz')\n",
    "        data['cd'] = cd_levels\n",
    "        cycle_levels_imputed = utils.load_sparse_data(inputs_file + '_imputed_cycle.sparse.npz')\n",
    "        data['cycle_imputed'] = cycle_levels_imputed\n",
    "        cd_levels_imputed = utils.load_sparse_data(inputs_file + '_imputed_cd.sparse.npz')\n",
    "        data['cd_imputed'] = cd_levels_imputed\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0872380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_data_loaders(data,\n",
    "                           n_of_PCs=48,\n",
    "                           cite=True,\n",
    "                           cd=False,\n",
    "                           cycle=False,\n",
    "                           imputed=False,\n",
    "                           batch_size=2048,\n",
    "                           train=True):\n",
    "    \n",
    "    selected_data = []\n",
    "    selected_data.append(data['inputs'][:, :n_of_PCs])\n",
    "    if imputed:\n",
    "        if cd:\n",
    "            selected_data.append(data['cd_imputed'])\n",
    "        if cycle:\n",
    "            selected_data.append(data['cycle_imputed'])\n",
    "    else:\n",
    "        if cd:\n",
    "            selected_data.append(data['cd'])\n",
    "        if cycle:\n",
    "            selected_data.append(data['cycle'])\n",
    "\n",
    "    inputs = np.concatenate(selected_data, 1)\n",
    "    targets = data['targets']\n",
    "    \n",
    "    if train:\n",
    "        train_loader, val_loader = utils.make_loaders(inputs, targets, val_size=2048 * 2, batch_size=batch_size, num_workers=1)\n",
    "        return train_loader, val_loader\n",
    "    else:\n",
    "        test_loader = utils.make_loaders(inputs, batch_size=batch_size, num_workers=1)\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "340f400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, model_params, trainer_params, train_loader, val_loader):\n",
    "    torch.cuda.empty_cache()\n",
    "    trainer = utils.Trainer(**trainer_params)\n",
    "    trainer.add_train_loader(train_loader)\n",
    "    trainer.add_val_loader(val_loader)\n",
    "    trainer.add_model(model, model_params)\n",
    "    val_loss = trainer.fit(return_val_loss=True, verbose=False)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "284aeb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial, data, model, device, cite=True):\n",
    "    \n",
    "    model_params = {}\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 25)\n",
    "    model_params['layers_sizes'] = []\n",
    "    for layer in range(num_layers):\n",
    "        ls = trial.suggest_int(f\"ls_{layer}\", 10, 10000)\n",
    "        model_params['layers_sizes'].append(ls)\n",
    "    \n",
    "    model_params['concat_pos'] = trial.suggest_int(\"concat_pos\", 1, num_layers)\n",
    "    max_concat_pos = max(1, model_params['concat_pos']-1)\n",
    "    model_params['n_of_layers_to_concat'] = trial.suggest_int(\"n_of_layers_to_concat\", 1, max_concat_pos)\n",
    "    \n",
    "    model_params['dropout'] = trial.suggest_float(\"dropout\", 0, 1)\n",
    "    \n",
    "    \n",
    "    trainer_params = {}\n",
    "    trainer_params['device'] = device\n",
    "    \n",
    "    trainer_params[\"wd\"] = trial.suggest_float(\"wd\", 1e-7, 1e1, log=True)\n",
    "    trainer_params[\"lr\"] = trial.suggest_float(\"lr\", 1e-5, 1e1, log=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    trainer_params['max_epochs'] = trial.suggest_int(\"max_epochs\", 1, 50)\n",
    "    trainer_params['max_schedule_epoch'] = trial.suggest_int(\"max_schedule_epoch\", 1, trainer_params['max_epochs'])\n",
    "    if trainer_params['max_epochs'] != trainer_params['max_schedule_epoch']:\n",
    "        trainer_params['min_lr'] = trial.suggest_float(\"min_lr\", 1e-5, 1e-1, log=True)\n",
    "    \n",
    "    trainer_params['sparsity_beta'] = trial.suggest_float(\"sparsity_beta\", 1e-9, 10, log=True)\n",
    "    trainer_params['sparsity_rho'] = trial.suggest_float('sparsity_rho', 1e-5, 0.05, log=True)\n",
    "\n",
    "    # regularization\n",
    "    trainer_params['l1_weight'] = trial.suggest_float('l1_weight', 1e-6, 1, log=True)\n",
    "    trainer_params['l2_weight'] = trial.suggest_float('l2_weight', 1e-6, 1, log=True)\n",
    "    \n",
    "    trainer_params['use_swa'] = trial.suggest_categorical('use_swa', [True, False])\n",
    "    if trainer_params['use_swa']:\n",
    "        trainer_params['swa_start'] = trial.suggest_int(\"swa_start\", 1, trainer_params['max_epochs'])\n",
    "        trainer_params[\"swa_lr\"] = trial.suggest_float(\"swa_lr\", 1e-5, 1e1, log=True)\n",
    "    \n",
    "    if not trainer_params['use_swa']:\n",
    "        trainer_params['use_one_cycle'] = trial.suggest_categorical('use_one_cycle', [True, False])\n",
    "        \n",
    "    \n",
    "    n_of_PCs = trial.suggest_int(\"n_of_PCs\", 5, 512)\n",
    "    if cite:\n",
    "        cd = trial.suggest_categorical('cd', [True, False])\n",
    "        cycle = trial.suggest_categorical('cycle', [True, False])\n",
    "        imputed = trial.suggest_categorical('imputed', [True, False])\n",
    "    \n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [512, 1024, 2048, 4096])\n",
    "    \n",
    "    train_loader, val_loader = configure_data_loaders(data,\n",
    "                                                       n_of_PCs=n_of_PCs,\n",
    "                                                       cite=cite,\n",
    "                                                       cd=cd,\n",
    "                                                       cycle=cycle,\n",
    "                                                       imputed=imputed,\n",
    "                                                       batch_size=batch_size)\n",
    "    \n",
    "    val_loss = run_model(model, model_params, trainer_params, train_loader, val_loader)\n",
    "    \n",
    "    return -val_loss[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "474c1a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/artemy/multimodal_proj/data/tuning_data/'\n",
    "\n",
    "data = load_data(data_dir, train=True, cite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e59a4a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-26 01:07:55,006]\u001b[0m A new study created in memory with name: no-name-4dc25676-b0f4-4aea-bf57-acec081ed3cc\u001b[0m\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:09<00:00,  1.24s/it]\n",
      "\u001b[32m[I 2022-10-26 01:08:07,310]\u001b[0m Trial 0 finished with value: -0.7495240569114685 and parameters: {'num_layers': 5, 'ls_0': 4700, 'ls_1': 346, 'ls_2': 7351, 'ls_3': 490, 'ls_4': 2659, 'concat_pos': 2, 'n_of_layers_to_concat': 1, 'dropout': 0.8167667578685678, 'wd': 1.8049363774415097, 'lr': 0.10436849345463711, 'max_epochs': 8, 'max_schedule_epoch': 8, 'sparsity_beta': 0.0010258683293824704, 'sparsity_rho': 0.0300673129187057, 'l1_weight': 1.7333063763915774e-06, 'l2_weight': 0.007351678753967521, 'use_swa': False, 'use_one_cycle': False, 'n_of_PCs': 97, 'cd': True, 'cycle': False, 'imputed': True, 'batch_size': 2048}. Best is trial 0 with value: -0.7495240569114685.\u001b[0m\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [01:15<00:00, 12.53s/it]\n",
      "\u001b[32m[I 2022-10-26 01:09:24,835]\u001b[0m Trial 1 finished with value: -0.4297490119934082 and parameters: {'num_layers': 10, 'ls_0': 1137, 'ls_1': 1646, 'ls_2': 9540, 'ls_3': 737, 'ls_4': 3259, 'ls_5': 8705, 'ls_6': 6035, 'ls_7': 951, 'ls_8': 8330, 'ls_9': 5037, 'concat_pos': 9, 'n_of_layers_to_concat': 3, 'dropout': 0.8631913965135128, 'wd': 0.0004235185403822565, 'lr': 0.380959635993172, 'max_epochs': 6, 'max_schedule_epoch': 4, 'min_lr': 0.004541102376515778, 'sparsity_beta': 0.0933712996425592, 'sparsity_rho': 1.5849077608454328e-05, 'l1_weight': 6.253114461904363e-06, 'l2_weight': 1.3826750079554728e-06, 'use_swa': True, 'swa_start': 2, 'swa_lr': 2.1576849935990046, 'n_of_PCs': 335, 'cd': False, 'cycle': False, 'imputed': True, 'batch_size': 1024}. Best is trial 0 with value: -0.7495240569114685.\u001b[0m\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:43<00:00,  2.90s/it]\n",
      "\u001b[32m[I 2022-10-26 01:10:08,923]\u001b[0m Trial 2 finished with value: -0.5783710330724716 and parameters: {'num_layers': 3, 'ls_0': 1280, 'ls_1': 4259, 'ls_2': 7971, 'concat_pos': 3, 'n_of_layers_to_concat': 1, 'dropout': 0.7719860995938754, 'wd': 1.2878990422767949, 'lr': 0.005776705661233245, 'max_epochs': 15, 'max_schedule_epoch': 9, 'min_lr': 2.6130521395768414e-05, 'sparsity_beta': 1.100938774679303e-09, 'sparsity_rho': 0.003332929744430053, 'l1_weight': 3.3014955570063082e-06, 'l2_weight': 0.007784657964791252, 'use_swa': True, 'swa_start': 5, 'swa_lr': 1.0619808788848015, 'n_of_PCs': 195, 'cd': False, 'cycle': True, 'imputed': True, 'batch_size': 1024}. Best is trial 0 with value: -0.7495240569114685.\u001b[0m\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:40<00:00, 33.62s/it]\n",
      "\u001b[32m[I 2022-10-26 01:11:56,279]\u001b[0m Trial 3 finished with value: -0.0 and parameters: {'num_layers': 24, 'ls_0': 7996, 'ls_1': 3603, 'ls_2': 7425, 'ls_3': 3845, 'ls_4': 5072, 'ls_5': 9420, 'ls_6': 753, 'ls_7': 7263, 'ls_8': 8577, 'ls_9': 8118, 'ls_10': 832, 'ls_11': 5472, 'ls_12': 9353, 'ls_13': 1583, 'ls_14': 5886, 'ls_15': 8025, 'ls_16': 456, 'ls_17': 3854, 'ls_18': 5324, 'ls_19': 5624, 'ls_20': 1558, 'ls_21': 4823, 'ls_22': 5258, 'ls_23': 3465, 'concat_pos': 13, 'n_of_layers_to_concat': 3, 'dropout': 0.9995215523802149, 'wd': 3.0491493859962805e-05, 'lr': 9.566614641673823e-05, 'max_epochs': 3, 'max_schedule_epoch': 2, 'min_lr': 0.00010555894773550755, 'sparsity_beta': 0.5402269561148851, 'sparsity_rho': 0.001472275444558984, 'l1_weight': 4.092575487736216e-05, 'l2_weight': 0.013613524531911916, 'use_swa': True, 'swa_start': 2, 'swa_lr': 0.01195784592356837, 'n_of_PCs': 133, 'cd': False, 'cycle': True, 'imputed': False, 'batch_size': 2048}. Best is trial 0 with value: -0.7495240569114685.\u001b[0m\n",
      "  0%|                                                                                                            | 0/24 [00:00<?, ?it/s]\n",
      "\u001b[33m[W 2022-10-26 01:12:05,035]\u001b[0m Trial 4 failed because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 210.00 MiB (GPU 0; 23.69 GiB total capacity; 21.59 GiB already allocated; 14.56 MiB free; 22.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/artemy/bin/miniconda3/envs/torch/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_2584841/4054403005.py\", line 2, in <lambda>\n",
      "    initialized_objective = lambda x: objective(x, model=Universal_Model, data=data, device=device, cite=True)\n",
      "  File \"/tmp/ipykernel_2584841/3248361061.py\", line 66, in objective\n",
      "    val_loss = run_model(model, model_params, trainer_params, train_loader, val_loader)\n",
      "  File \"/tmp/ipykernel_2584841/4097413200.py\", line 7, in run_model\n",
      "    val_loss = trainer.fit(return_val_loss=True, verbose=False)\n",
      "  File \"/home/artemy/multimodal_proj/nb/utils.py\", line 266, in fit\n",
      "    self.fit_epoch()\n",
      "  File \"/home/artemy/multimodal_proj/nb/utils.py\", line 292, in fit_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/artemy/bin/miniconda3/envs/torch/lib/python3.9/site-packages/torch/_tensor.py\", line 396, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/home/artemy/bin/miniconda3/envs/torch/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 173, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 210.00 MiB (GPU 0; 23.69 GiB total capacity; 21.59 GiB already allocated; 14.56 MiB free; 22.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 210.00 MiB (GPU 0; 23.69 GiB total capacity; 21.59 GiB already allocated; 14.56 MiB free; 22.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study()\n\u001b[1;32m      2\u001b[0m initialized_objective \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: objective(x, model\u001b[38;5;241m=\u001b[39mUniversal_Model, data\u001b[38;5;241m=\u001b[39mdata, device\u001b[38;5;241m=\u001b[39mdevice, cite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitialized_objective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/torch/lib/python3.9/site-packages/optuna/study/study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/torch/lib/python3.9/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/torch/lib/python3.9/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/torch/lib/python3.9/site-packages/optuna/study/_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    230\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    233\u001b[0m ):\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/torch/lib/python3.9/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study()\n\u001b[0;32m----> 2\u001b[0m initialized_objective \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUniversal_Model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(initialized_objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, data, model, device, cite)\u001b[0m\n\u001b[1;32m     56\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_categorical(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m4096\u001b[39m])\n\u001b[1;32m     58\u001b[0m train_loader, val_loader \u001b[38;5;241m=\u001b[39m configure_data_loaders(data,\n\u001b[1;32m     59\u001b[0m                                                    n_of_PCs\u001b[38;5;241m=\u001b[39mn_of_PCs,\n\u001b[1;32m     60\u001b[0m                                                    cite\u001b[38;5;241m=\u001b[39mcite,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m                                                    imputed\u001b[38;5;241m=\u001b[39mimputed,\n\u001b[1;32m     64\u001b[0m                                                    batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m---> 66\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mval_loss[\u001b[38;5;241m1\u001b[39m]\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(model, model_params, trainer_params, train_loader, val_loader)\u001b[0m\n\u001b[1;32m      5\u001b[0m trainer\u001b[38;5;241m.\u001b[39madd_val_loader(val_loader)\n\u001b[1;32m      6\u001b[0m trainer\u001b[38;5;241m.\u001b[39madd_model(model, model_params)\n\u001b[0;32m----> 7\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_val_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m val_loss\n",
      "File \u001b[0;32m~/multimodal_proj/nb/utils.py:266\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, do_validation, calculate_cor, subset_train, return_val_loss, verbose)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_batch_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_batch_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_swa:\n",
      "File \u001b[0;32m~/multimodal_proj/nb/utils.py:292\u001b[0m, in \u001b[0;36mTrainer.fit_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_rna \u001b[38;5;241m+\u001b[39m l1_norm \u001b[38;5;241m+\u001b[39m l2_norm \u001b[38;5;241m+\u001b[39m sparsity_loss\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 292\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_schedule_epoch \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_one_cycle:\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/torch/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/torch/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 210.00 MiB (GPU 0; 23.69 GiB total capacity; 21.59 GiB already allocated; 14.56 MiB free; 22.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "study = optuna.create_study()\n",
    "initialized_objective = lambda x: objective(x, model=Universal_Model, data=data, device=device, cite=True)\n",
    "study.optimize(initialized_objective, n_trials=1000)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fcbd4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc45817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
